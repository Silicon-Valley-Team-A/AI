# -*- coding: utf-8 -*-
"""search_project.ipynb
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c-oIMSgOSd49GdLhfdvVKZErxTvhwbSx
"""
#!pip install ftfy regex tqdm
#!pip install git+https://github.com/openai/CLIP.git
#!pip install spotipy

#from google.colab import drive
#drive.mount('/content/drive')

#@title
# -*- coding: utf-8 -*-
"""1. Classification_CLIP.ipynb의 사본​
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Z9lzUs28ZEQPUfyhISazMwK-gKQCSek
"""
# !conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0
from spotipy.util import CLIENT_CREDS_ENV_VARS
import spotipy
from spotipy.oauth2 import SpotifyClientCredentials

birdy_uri = 'spotify:artist:2WX2uTcsvV5OnS0inACecP'
cid = 'your_id'
secret = 'your_secret'
client_credentials_manager = SpotifyClientCredentials(client_id=cid, client_secret=secret)
sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)
"""## Import Libraries"""

import os
import PIL
import clip
import torch
import csv
import numpy as np
import torchvision
import urllib.request
import json
from torchvision import transforms

import torch.nn as nn #추가
import torch.nn.functional as F #추가

class Net(nn.Module): #추가
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, 3)#입력 채널(흑백:1 RGB:3), 출력 채널=필터개수, 필터 사이즈 필터사이즈^2*출력채널*입력채널+출력채널 = 파라미터 수
        self.conv2 = nn.Conv2d(32, 32, 3)#출력채널=입력채널 배치사이즈만큼 출력채널이 늘어나게 된다.
        self.pool = nn.MaxPool2d(2)
        self.dropout1 = nn.Dropout(0.25)
        self.conv3 = nn.Conv2d(32, 64, 3)
        self.conv4 = nn.Conv2d(64, 64, 3)#9
        self.fc1 = nn.Linear(4096, 512)#입력 채널, 출력채널 입력채널*출력채널+출력채널 = 파라미터
        self.fc2 = nn.Linear(512, 64)
        self.dropout2 = nn.Dropout(0.5)
        self.fc3 = nn.Linear(64, 3)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.dropout1(x)
        x = F.relu(self.conv3(x))
        x = self.pool(F.relu(self.conv4(x)))
        x = self.dropout1(x)
        x = F.relu(self.conv4(x))
        x = self.pool(F.relu(self.conv4(x)))
        x = self.dropout1(x)
        x = F.relu(self.conv4(x))
        x = self.pool(F.relu(self.conv4(x)))
        x = self.dropout1(x)
        if(batch_size > 1):
            x = torch.flatten(x, 1)
        else:
            x = torch.flatten(x)
        x = F.relu(self.fc1(x))
        x = self.dropout2(x)
        x = F.relu(self.fc2(x))
        x = self.dropout2(x)
        x = F.softmax(self.fc3(x), dim=0)
        return x


#폴더 설정
os.chdir("C:/Users/gycej/Downloads/AI_online_internship_program/ai")

f = open('./keywords.csv', 'r', encoding='utf-8')
rdr = csv.reader(f)
text = []
change = []
for line in rdr:
    text.append(line[0])
    change.append(line[1])
f.close()   

#print(change)
"""## Model"""

# Load the model
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
#device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load('ViT-B/32', device)

"""## Zero-shot Prediction"""
#print(text)
#for c in cifar100.classes:
#  print(f"a photo of a {c}")

#text = ["Amusement", "Anger", "Awe", "Contentment", "Disgust", "Excitement", "Fear", "Sad"]#인식될 키워드 리스트 수정바람
classes = ("happy", "sad", "scary")

# Prepare the inputs
image = PIL.Image.open('./img/camping.jpg')
#display(image)
image_input = preprocess(image).unsqueeze(0).to(device)
text_inputs = torch.cat([clip.tokenize(f"a photo of a {c}") for c in text]).to(device)

PATH = './moodmodel.pth'
batch_size = 1


net = Net().to(device)
net.load_state_dict(torch.load(PATH, map_location = device))
net.to(device)
transform = transforms.Compose(
    [
        transforms.Resize((196, 196)),
        transforms.ToTensor(),#데이터 전처리 필요
    ]
)
outputs = net(transform(image).to(device))
#print(numpy.argmax(numpy.array(outputs.tolist())))
predicted = np.argmax(np.array(outputs.tolist()))
print("mood: " + classes[predicted])
mood = classes[predicted]
# Calculate features
with torch.no_grad():
    image_features = model.encode_image(image_input)
    text_features = model.encode_text(text_inputs)

# Pick the top 5 most similar labels for the image
image_features /= image_features.norm(dim=-1, keepdim=True)
text_features /= text_features.norm(dim=-1, keepdim=True)
similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)

#print(100.0 * image_features @ text_features.T)

values, indices = similarity[0].topk(5)


# Print the result
#print("\nTop predictions:\n")
n = 0
for value, index in zip(values, indices):
    if n==0:
        keyword = text[index]
        
    n= n+1
    print(f"{text[index]:>16s}: {100 * value.item():.2f}%%")

print("keyword: "+keyword)
#print(keyword)
#results = sp.search(q="Music for camping"+" genre: pop", limit=20)
#results = sp.search(q="우울할 때 듣는 음악", limit=20)
#results = sp.search(q="Music for "+keyword, limit=20)
#results = sp.search(q=keyword+" music", limit=20)
genre = None #hip-hop, pop, rock .....

if(genre == None):
  results = sp.search(q=keyword, limit=50)
  #results = sp.search(q="song when anger", limit=20)
  #print("test1")
else:
  results = sp.search(q=keyword+" genre: "+genre, limit=50)
#print("test2")
#print(results['tracks']['items'][0]['preview_url'])
#print(results['tracks']['items'][0]['album']['images'][0]['url'])
#print(results['tracks']['items'][0]['artists'][0]['name'])
#print(results['tracks']['items'][0]['album']['name'])
#print(results['tracks']['items'][0]['id'])
#print(results['tracks']['items'][0]['duration_ms'])
#print(results)

search_result = {}
search_result['song'] = []

try:
    if not os.path.exists("./music_image"):
        os.makedirs("./music_image")
except OSError:
    print('Error: Creating directory.'+"./music_image")
os.chdir("./music_image")
def search_songs(results):
    
  data = {}
  data['song'] = []
  for idx, track in enumerate(results['tracks']['items']):
      try:
        #print(idx, track['name'], track['preview_url'], track['album']['images'][0]['url'], track['artists'][0]['name'], track['album']['name'], track['id'], track['duration_ms'])
        urllib.request.urlretrieve(track['album']['images'][0]['url'], track['name']+".jpg")
        #img = PIL.Image.open(track['name']+".jpg")
        #display(img)
        if(not (track['preview_url']==None)):
          data['song'].append({
              "title": track['name'],
              "image_album":  track['album']['images'][0]['url'],
              "file": track['preview_url'],
              "artist": track['artists'][0]['name'],
              "title_album": track['album']['name'],
              "id": track['id'],
              "duration": track['duration_ms']
          })
      except:
        pass
  return data

def get_features(track_id):
    # get track id information
    #track_info = sp.search(q=song, type='track')
    #track_id = track_info["tracks"]["items"][0]["id"]

    # get audio_feature
    features = sp.audio_features(tracks=[track_id])
    #print(features)
    try:
      #acousticness = features[0]["acousticness"]
      danceability = features[0]["danceability"]
      energy = features[0]["energy"]
      #liveness = features[0]["liveness"]
      #loudness = features[0]["loudness"]
      valence = features[0]["valence"]
      #mode = features[0]["mode"]
      tempo = features[0]["tempo"]

      result = {"danceability" : danceability,
                  "energy" : energy,
                  "valence" : valence,
                  "tempo" : tempo}
    except:
      pass

    return result
    

def music_classification(data, search_result):
  cnt = 0
  for classify in data['song']:
      result = get_features(classify['id'])
      cnt += 1
      if(mood=="happy"):
        if(result['danceability']>0.7 and result['energy']>0.7 and result['valence']>0.7 and result['tempo']>100):
          #print(classify)
          search_result['song'].append(classify)
        else:
          pass
      elif(mood=="sad"):
        if(result['danceability']<0.4 and result['energy']<0.4 and result['valence']<0.4 and result['tempo']<90):
          #print(classify)
          search_result['song'].append(classify)
        else:
          pass
      elif(mood=="scary"):
        if(result['danceability']<0.7 and result['danceability']>0.4 and result['energy']>0.7 and result['valence']<0.4 and result['tempo']>100):
          #print(classify)
          search_result['song'].append(classify)
        else:
          pass
  return search_result

data = search_songs(results)

#print(data)
#print(data['song'][0]['id'])
#print(get_features(data['song'][0]['title']))
search_result = music_classification(data, search_result)


if(len(search_result['song']) < 10):
  results = sp.search(q="song",limit=50)
  #print(results)

  data = search_songs(results)
  search_result = music_classification(data, search_result)

  #print(search_result)
  #print(len(search_result['song']))
  if(len(search_result['song']) < 10):
    results = sp.search(q="music",limit=50)
    #print(results)

    data = search_songs(results)
    search_result = music_classification(data, search_result)

    #print(search_result)
    #print(len(search_result['song']))
    if(len(search_result['song']) < 10):
      results = sp.search(q=mood,limit=50)
      #print(results)

      data = search_songs(results)
      search_result = music_classification(data, search_result)

      #print(search_result)
      #print(len(search_result['song']))
      if(len(search_result['song']) < 10):
        results = sp.search(q="songs",limit=50)
        #print(results)

        data = search_songs(results)
        search_result = music_classification(data, search_result)

        #print(search_result)
        #print(len(search_result['song']))
        

      
#print(search_result)
print(len(search_result['song']))
with open("./result.json", 'w') as outfile:
  json.dump(search_result, outfile, indent=8)
